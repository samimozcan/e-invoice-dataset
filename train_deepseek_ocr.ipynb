{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DeepSeek OCR Fine-tuning for Invoice Extraction\n",
        "\n",
        "This notebook fine-tunes the DeepSeek OCR model on Turkish e-invoice documents.\n",
        "\n",
        "## Key Features:\n",
        "- **QLoRA** for memory-efficient training on RTX 4090 (24GB VRAM)\n",
        "- **4-bit quantization** with bitsandbytes\n",
        "- **Gradient checkpointing** for reduced memory\n",
        "- **Multi-page invoice support** - DeepSeek OCR handles multiple images per sample\n",
        "\n",
        "## Requirements:\n",
        "- RTX 4090 (24GB VRAM) or equivalent\n",
        "- CUDA 11.8+\n",
        "- Flash Attention 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install requirements (run this first on Vast.ai)\n",
        "!pip install -q torch transformers>=4.40.0 accelerate peft>=0.10.0 bitsandbytes>=0.43.0 wandb scikit-learn tqdm pillow torchvision\n",
        "!pip install -q flash-attn --no-build-isolation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import (\n",
        "    Trainer, \n",
        "    TrainingArguments, \n",
        "    AutoTokenizer, \n",
        "    AutoModelForCausalLM,\n",
        "    BitsAndBytesConfig,\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n",
        "from PIL import Image, ImageOps\n",
        "from torchvision import transforms\n",
        "from typing import List, Dict, Optional, Tuple, Union\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Check GPU\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration\n",
        "\n",
        "Adjust these settings based on your GPU and requirements."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============== CONFIGURATION ==============\n",
        "# Model\n",
        "MODEL_NAME = \"deepseek-ai/DeepSeek-OCR\"\n",
        "OUTPUT_DIR = \"./deepseek_ocr_finetuned\"\n",
        "\n",
        "# Dataset\n",
        "DATASET_PATH = \"dataset_multi_image.jsonl\"\n",
        "IMAGES_DIR = \"images\"\n",
        "\n",
        "# Training hyperparameters (optimized for RTX 4090 24GB)\n",
        "BATCH_SIZE = 1\n",
        "GRADIENT_ACCUMULATION_STEPS = 8  # Effective batch size = 8\n",
        "NUM_EPOCHS = 10  # More epochs for small dataset\n",
        "LEARNING_RATE = 2e-4  # Higher LR for LoRA\n",
        "WARMUP_RATIO = 0.1\n",
        "MAX_SEQ_LENGTH = 4096\n",
        "\n",
        "# LoRA configuration\n",
        "LORA_R = 16\n",
        "LORA_ALPHA = 32\n",
        "LORA_DROPOUT = 0.05\n",
        "\n",
        "# Quantization (set to True for 4-bit, False for bf16 full precision)\n",
        "USE_4BIT_QUANTIZATION = True\n",
        "\n",
        "# Logging\n",
        "USE_WANDB = False  # Set to True to enable Weights & Biases logging\n",
        "WANDB_PROJECT = \"deepseek-ocr-invoice\"\n",
        "\n",
        "# Image processing\n",
        "IMAGE_SIZE = 640\n",
        "BASE_SIZE = 1024\n",
        "PATCH_SIZE = 16\n",
        "DOWNSAMPLE_RATIO = 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============== IMAGE PROCESSING HELPERS ==============\n",
        "\n",
        "def load_image(image_path: str) -> Optional[Image.Image]:\n",
        "    \"\"\"Load image with EXIF orientation correction.\"\"\"\n",
        "    try:\n",
        "        image = Image.open(image_path)\n",
        "        corrected_image = ImageOps.exif_transpose(image)\n",
        "        return corrected_image.convert(\"RGB\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading image {image_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "def find_closest_aspect_ratio(aspect_ratio, target_ratios, width, height, image_size):\n",
        "    \"\"\"Find the closest aspect ratio from target ratios.\"\"\"\n",
        "    best_ratio_diff = float('inf')\n",
        "    best_ratio = (1, 1)\n",
        "    area = width * height\n",
        "    for ratio in target_ratios:\n",
        "        target_aspect_ratio = ratio[0] / ratio[1]\n",
        "        ratio_diff = abs(aspect_ratio - target_aspect_ratio)\n",
        "        if ratio_diff < best_ratio_diff:\n",
        "            best_ratio_diff = ratio_diff\n",
        "            best_ratio = ratio\n",
        "        elif ratio_diff == best_ratio_diff:\n",
        "            if area > 0.5 * image_size * image_size * ratio[0] * ratio[1]:\n",
        "                best_ratio = ratio\n",
        "    return best_ratio\n",
        "\n",
        "def dynamic_preprocess(image, min_num=1, max_num=9, image_size=640, use_thumbnail=False):\n",
        "    \"\"\"Dynamically preprocess image into tiles based on aspect ratio.\"\"\"\n",
        "    orig_width, orig_height = image.size\n",
        "    aspect_ratio = orig_width / orig_height\n",
        "\n",
        "    target_ratios = set(\n",
        "        (i, j) for n in range(min_num, max_num + 1) \n",
        "        for i in range(1, n + 1) \n",
        "        for j in range(1, n + 1) \n",
        "        if i * j <= max_num and i * j >= min_num\n",
        "    )\n",
        "    target_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])\n",
        "\n",
        "    target_aspect_ratio = find_closest_aspect_ratio(\n",
        "        aspect_ratio, target_ratios, orig_width, orig_height, image_size\n",
        "    )\n",
        "\n",
        "    target_width = image_size * target_aspect_ratio[0]\n",
        "    target_height = image_size * target_aspect_ratio[1]\n",
        "    blocks = target_aspect_ratio[0] * target_aspect_ratio[1]\n",
        "\n",
        "    resized_img = image.resize((target_width, target_height))\n",
        "    processed_images = []\n",
        "    for i in range(blocks):\n",
        "        box = (\n",
        "            (i % (target_width // image_size)) * image_size,\n",
        "            (i // (target_width // image_size)) * image_size,\n",
        "            ((i % (target_width // image_size)) + 1) * image_size,\n",
        "            ((i // (target_width // image_size)) + 1) * image_size\n",
        "        )\n",
        "        split_img = resized_img.crop(box)\n",
        "        processed_images.append(split_img)\n",
        "    \n",
        "    assert len(processed_images) == blocks\n",
        "    if use_thumbnail and len(processed_images) != 1:\n",
        "        thumbnail_img = image.resize((image_size, image_size))\n",
        "        processed_images.append(thumbnail_img)\n",
        "    return processed_images, target_aspect_ratio\n",
        "\n",
        "class BasicImageTransform:\n",
        "    \"\"\"Basic image transformation with normalization.\"\"\"\n",
        "    def __init__(\n",
        "        self, \n",
        "        mean: Tuple[float, float, float] = (0.5, 0.5, 0.5),\n",
        "        std: Tuple[float, float, float] = (0.5, 0.5, 0.5),\n",
        "        normalize: bool = True\n",
        "    ):\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "        transform_pipelines = [transforms.ToTensor()]\n",
        "        if normalize:\n",
        "            transform_pipelines.append(transforms.Normalize(mean=mean, std=std))\n",
        "        self.transform = transforms.Compose(transform_pipelines)\n",
        "    \n",
        "    def __call__(self, x):\n",
        "        return self.transform(x)\n",
        "\n",
        "def text_encode(tokenizer, text: str, bos: bool = True, eos: bool = False):\n",
        "    \"\"\"Encode text with optional BOS/EOS tokens.\"\"\"\n",
        "    t = tokenizer.encode(text, add_special_tokens=False)\n",
        "    bos_id = tokenizer.bos_token_id if tokenizer.bos_token_id is not None else 1\n",
        "    eos_id = tokenizer.eos_token_id if tokenizer.eos_token_id is not None else 2\n",
        "    \n",
        "    if bos:\n",
        "        t = [bos_id] + t\n",
        "    if eos:\n",
        "        t = t + [eos_id]\n",
        "    return t\n",
        "\n",
        "print(\"‚úÖ Image processing helpers loaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset and Data Collator\n",
        "\n",
        "The dataset handles multi-page invoices where each sample can have multiple images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============== DATASET ==============\n",
        "\n",
        "class DeepSeekOCRDataset(Dataset):\n",
        "    \"\"\"Dataset for DeepSeek OCR fine-tuning with multi-image support.\"\"\"\n",
        "    \n",
        "    def __init__(self, data_path: str, images_dir: str = \"images\"):\n",
        "        self.data = []\n",
        "        self.images_dir = images_dir\n",
        "        \n",
        "        with open(data_path, 'r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                item = json.loads(line)\n",
        "                # Verify all images exist\n",
        "                valid = True\n",
        "                for img_path in item['images']:\n",
        "                    full_path = os.path.join(images_dir, os.path.basename(img_path)) if not os.path.exists(img_path) else img_path\n",
        "                    if not os.path.exists(full_path):\n",
        "                        print(f\"‚ö†Ô∏è Missing image: {img_path}\")\n",
        "                        valid = False\n",
        "                        break\n",
        "                if valid:\n",
        "                    self.data.append(item)\n",
        "        \n",
        "        print(f\"‚úÖ Loaded {len(self.data)} samples from {data_path}\")\n",
        "                \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "\n",
        "class DeepSeekOCRDataCollator:\n",
        "    \"\"\"Collator for DeepSeek OCR that handles multi-image inputs.\"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self, \n",
        "        tokenizer, \n",
        "        image_size: int = IMAGE_SIZE, \n",
        "        base_size: int = BASE_SIZE, \n",
        "        patch_size: int = PATCH_SIZE, \n",
        "        downsample_ratio: int = DOWNSAMPLE_RATIO,\n",
        "        max_seq_length: int = MAX_SEQ_LENGTH\n",
        "    ):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.image_size = image_size\n",
        "        self.base_size = base_size\n",
        "        self.patch_size = patch_size\n",
        "        self.downsample_ratio = downsample_ratio\n",
        "        self.max_seq_length = max_seq_length\n",
        "        self.image_transform = BasicImageTransform(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5), normalize=True)\n",
        "        \n",
        "        # Get image token\n",
        "        self.image_token = '<image>'\n",
        "        self.image_token_id = tokenizer.convert_tokens_to_ids(self.image_token)\n",
        "        if self.image_token_id is None or self.image_token_id == tokenizer.unk_token_id:\n",
        "            self.image_token_id = 128815  # Fallback for DeepSeek OCR\n",
        "            \n",
        "        # Get BOS token\n",
        "        self.bos_token_id = tokenizer.bos_token_id if tokenizer.bos_token_id is not None else 1\n",
        "        self.eos_token_id = tokenizer.eos_token_id if tokenizer.eos_token_id is not None else 2\n",
        "        self.pad_token_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else 0\n",
        "             \n",
        "    def __call__(self, batch):\n",
        "        input_ids_batch = []\n",
        "        labels_batch = []\n",
        "        images_batch = []\n",
        "        images_seq_mask_batch = []\n",
        "        images_spatial_crop_batch = []\n",
        "        \n",
        "        for item in batch:\n",
        "            prompt = item['prompt']\n",
        "            response = item['response']\n",
        "            image_paths = item['images']\n",
        "            \n",
        "            # Load images\n",
        "            images = []\n",
        "            for img_path in image_paths:\n",
        "                # Handle relative paths\n",
        "                if not os.path.exists(img_path):\n",
        "                    img_path = os.path.join(IMAGES_DIR, os.path.basename(img_path))\n",
        "                img = load_image(img_path)\n",
        "                if img:\n",
        "                    images.append(img)\n",
        "            \n",
        "            # Split prompt by image tokens\n",
        "            text_splits = prompt.split(self.image_token)\n",
        "            \n",
        "            tokenized_str = []\n",
        "            images_seq_mask = []\n",
        "            images_list = []\n",
        "            images_crop_list = []\n",
        "            current_images_spatial_crop = []\n",
        "            \n",
        "            for i, text_sep in enumerate(text_splits):\n",
        "                # Tokenize text segment\n",
        "                tokenized_sep = self.tokenizer.encode(text_sep, add_special_tokens=False)\n",
        "                tokenized_str += tokenized_sep\n",
        "                images_seq_mask += [False] * len(tokenized_sep)\n",
        "                \n",
        "                # Process image if available\n",
        "                if i < len(images):\n",
        "                    image = images[i]\n",
        "                    \n",
        "                    # Dynamic preprocessing for tiles\n",
        "                    images_crop_raw, crop_ratio = dynamic_preprocess(image, image_size=self.image_size)\n",
        "                    \n",
        "                    # Global view\n",
        "                    global_view = ImageOps.pad(\n",
        "                        image, \n",
        "                        (self.base_size, self.base_size),\n",
        "                        color=tuple(int(x * 255) for x in self.image_transform.mean)\n",
        "                    )\n",
        "                    images_list.append(self.image_transform(global_view).to(torch.bfloat16))\n",
        "                    \n",
        "                    width_crop_num, height_crop_num = crop_ratio\n",
        "                    current_images_spatial_crop.append([width_crop_num, height_crop_num])\n",
        "                    \n",
        "                    # Add crop tiles if needed\n",
        "                    if width_crop_num > 1 or height_crop_num > 1:\n",
        "                        for crop_img in images_crop_raw:\n",
        "                            images_crop_list.append(self.image_transform(crop_img).to(torch.bfloat16))\n",
        "                            \n",
        "                    # Calculate image token count\n",
        "                    num_queries = math.ceil((self.image_size // self.patch_size) / self.downsample_ratio)\n",
        "                    num_queries_base = math.ceil((self.base_size // self.patch_size) / self.downsample_ratio)\n",
        "                    \n",
        "                    # Base image tokens\n",
        "                    tokenized_image = ([self.image_token_id] * num_queries_base + [self.image_token_id]) * num_queries_base\n",
        "                    tokenized_image += [self.image_token_id]\n",
        "                    \n",
        "                    # Crop tokens if applicable\n",
        "                    if width_crop_num > 1 or height_crop_num > 1:\n",
        "                        tokenized_image += ([self.image_token_id] * (num_queries * width_crop_num) + [self.image_token_id]) * (\n",
        "                            num_queries * height_crop_num\n",
        "                        )\n",
        "                                    \n",
        "                    tokenized_str += tokenized_image\n",
        "                    images_seq_mask += [True] * len(tokenized_image)\n",
        "            \n",
        "            # Add BOS token at the beginning\n",
        "            tokenized_str = [self.bos_token_id] + tokenized_str\n",
        "            images_seq_mask = [False] + images_seq_mask\n",
        "            \n",
        "            # Tokenize response\n",
        "            response_tokens = self.tokenizer.encode(response, add_special_tokens=False) + [self.eos_token_id]\n",
        "            \n",
        "            # Combine input_ids\n",
        "            input_ids = tokenized_str + response_tokens\n",
        "            images_seq_mask += [False] * len(response_tokens)\n",
        "            \n",
        "            # Create labels (-100 for prompt tokens, actual tokens for response)\n",
        "            labels = [-100] * len(tokenized_str) + response_tokens\n",
        "            \n",
        "            # Truncate if necessary\n",
        "            if len(input_ids) > self.max_seq_length:\n",
        "                input_ids = input_ids[:self.max_seq_length]\n",
        "                labels = labels[:self.max_seq_length]\n",
        "                images_seq_mask = images_seq_mask[:self.max_seq_length]\n",
        "            \n",
        "            input_ids_batch.append(torch.LongTensor(input_ids))\n",
        "            labels_batch.append(torch.LongTensor(labels))\n",
        "            images_seq_mask_batch.append(torch.tensor(images_seq_mask, dtype=torch.bool))\n",
        "            \n",
        "            if len(images_list) > 0:\n",
        "                images_ori = torch.stack(images_list, dim=0)\n",
        "                images_spatial_crop_tensor = torch.tensor(current_images_spatial_crop, dtype=torch.long)\n",
        "                if images_crop_list:\n",
        "                    images_crop = torch.stack(images_crop_list, dim=0)\n",
        "                else:\n",
        "                    images_crop = torch.zeros((1, 3, self.image_size, self.image_size), dtype=torch.bfloat16)\n",
        "                \n",
        "                images_batch.append((images_crop, images_ori))\n",
        "                images_spatial_crop_batch.append(images_spatial_crop_tensor)\n",
        "\n",
        "        # Pad sequences\n",
        "        input_ids_padded = torch.nn.utils.rnn.pad_sequence(\n",
        "            input_ids_batch, batch_first=True, padding_value=self.pad_token_id\n",
        "        )\n",
        "        labels_padded = torch.nn.utils.rnn.pad_sequence(\n",
        "            labels_batch, batch_first=True, padding_value=-100\n",
        "        )\n",
        "        images_seq_mask_padded = torch.nn.utils.rnn.pad_sequence(\n",
        "            images_seq_mask_batch, batch_first=True, padding_value=False\n",
        "        )\n",
        "        \n",
        "        return {\n",
        "            \"input_ids\": input_ids_padded,\n",
        "            \"labels\": labels_padded,\n",
        "            \"images\": images_batch,\n",
        "            \"images_seq_mask\": images_seq_mask_padded,\n",
        "            \"images_spatial_crop\": images_spatial_crop_batch\n",
        "        }\n",
        "\n",
        "print(\"‚úÖ Dataset and Collator classes defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Custom Trainer for DeepSeek OCR\n",
        "\n",
        "We need a custom trainer to properly handle the multi-image inputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============== CUSTOM TRAINER ==============\n",
        "\n",
        "class DeepSeekOCRTrainer(Trainer):\n",
        "    \"\"\"Custom trainer that handles DeepSeek OCR's multi-image inputs.\"\"\"\n",
        "    \n",
        "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
        "        # Extract image-related inputs\n",
        "        images = inputs.pop(\"images\", None)\n",
        "        images_seq_mask = inputs.pop(\"images_seq_mask\", None)\n",
        "        images_spatial_crop = inputs.pop(\"images_spatial_crop\", None)\n",
        "        \n",
        "        # Forward pass with all inputs\n",
        "        outputs = model(\n",
        "            input_ids=inputs[\"input_ids\"],\n",
        "            labels=inputs[\"labels\"],\n",
        "            images=images,\n",
        "            images_seq_mask=images_seq_mask,\n",
        "            images_spatial_crop=images_spatial_crop,\n",
        "        )\n",
        "        \n",
        "        loss = outputs.loss\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "print(\"‚úÖ Custom Trainer defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============== LOAD MODEL AND TOKENIZER ==============\n",
        "\n",
        "print(f\"Loading model: {MODEL_NAME}\")\n",
        "print(f\"Using 4-bit quantization: {USE_4BIT_QUANTIZATION}\")\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
        "\n",
        "# Ensure pad token is set\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "# Quantization config for 4-bit (QLoRA)\n",
        "if USE_4BIT_QUANTIZATION:\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "    )\n",
        "    \n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        trust_remote_code=True,\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=\"auto\",\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        attn_implementation='flash_attention_2',\n",
        "    )\n",
        "    \n",
        "    # Prepare model for k-bit training\n",
        "    model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True)\n",
        "else:\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        trust_remote_code=True,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        device_map=\"auto\",\n",
        "        attn_implementation='flash_attention_2',\n",
        "    )\n",
        "    model.gradient_checkpointing_enable()\n",
        "\n",
        "print(f\"‚úÖ Model loaded successfully\")\n",
        "print(f\"Model type: {type(model).__name__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============== APPLY LoRA ==============\n",
        "\n",
        "# Find all linear layers for LoRA\n",
        "def find_all_linear_names(model):\n",
        "    \"\"\"Find all linear layer names for LoRA targeting.\"\"\"\n",
        "    lora_module_names = set()\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, torch.nn.Linear):\n",
        "            names = name.split('.')\n",
        "            lora_module_names.add(names[-1])\n",
        "    \n",
        "    # Remove output layer if present\n",
        "    if 'lm_head' in lora_module_names:\n",
        "        lora_module_names.remove('lm_head')\n",
        "    \n",
        "    return list(lora_module_names)\n",
        "\n",
        "# Get target modules\n",
        "target_modules = find_all_linear_names(model)\n",
        "print(f\"Target modules for LoRA: {target_modules}\")\n",
        "\n",
        "# LoRA configuration\n",
        "lora_config = LoraConfig(\n",
        "    r=LORA_R,\n",
        "    lora_alpha=LORA_ALPHA,\n",
        "    target_modules=target_modules,\n",
        "    lora_dropout=LORA_DROPOUT,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        ")\n",
        "\n",
        "# Apply LoRA\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Print trainable parameters\n",
        "def print_trainable_parameters(model):\n",
        "    \"\"\"Print the number of trainable parameters.\"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"Trainable params: {trainable_params:,} || \"\n",
        "        f\"All params: {all_param:,} || \"\n",
        "        f\"Trainable %: {100 * trainable_params / all_param:.2f}%\"\n",
        "    )\n",
        "\n",
        "print_trainable_parameters(model)\n",
        "print(\"‚úÖ LoRA applied successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============== LOAD DATASET ==============\n",
        "\n",
        "# Load dataset\n",
        "dataset = DeepSeekOCRDataset(DATASET_PATH, images_dir=IMAGES_DIR)\n",
        "\n",
        "# Create data collator\n",
        "collator = DeepSeekOCRDataCollator(\n",
        "    tokenizer=tokenizer,\n",
        "    image_size=IMAGE_SIZE,\n",
        "    base_size=BASE_SIZE,\n",
        "    patch_size=PATCH_SIZE,\n",
        "    downsample_ratio=DOWNSAMPLE_RATIO,\n",
        "    max_seq_length=MAX_SEQ_LENGTH,\n",
        ")\n",
        "\n",
        "# Test collator with one sample\n",
        "print(\"\\nüìã Testing collator with first sample...\")\n",
        "test_batch = collator([dataset[0]])\n",
        "print(f\"  Input IDs shape: {test_batch['input_ids'].shape}\")\n",
        "print(f\"  Labels shape: {test_batch['labels'].shape}\")\n",
        "print(f\"  Images seq mask shape: {test_batch['images_seq_mask'].shape}\")\n",
        "print(f\"  Number of images: {len(test_batch['images'])}\")\n",
        "if len(test_batch['images']) > 0:\n",
        "    print(f\"  Image crops shape: {test_batch['images'][0][0].shape}\")\n",
        "    print(f\"  Image ori shape: {test_batch['images'][0][1].shape}\")\n",
        "print(\"‚úÖ Collator test passed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============== TRAINING ARGUMENTS ==============\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    \n",
        "    # Batch size settings\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
        "    \n",
        "    # Training settings\n",
        "    num_train_epochs=NUM_EPOCHS,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    warmup_ratio=WARMUP_RATIO,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    \n",
        "    # Precision\n",
        "    bf16=True,\n",
        "    \n",
        "    # Memory optimization\n",
        "    gradient_checkpointing=True,\n",
        "    optim=\"adamw_torch_fused\",  # Faster optimizer\n",
        "    \n",
        "    # Logging\n",
        "    logging_steps=1,\n",
        "    logging_first_step=True,\n",
        "    report_to=\"wandb\" if USE_WANDB else \"none\",\n",
        "    \n",
        "    # Saving\n",
        "    save_steps=50,\n",
        "    save_total_limit=3,\n",
        "    \n",
        "    # Other\n",
        "    dataloader_pin_memory=False,\n",
        "    remove_unused_columns=False,\n",
        "    dataloader_num_workers=0,  # Avoid multiprocessing issues\n",
        "    \n",
        "    # Disable find_unused_parameters for memory efficiency\n",
        "    ddp_find_unused_parameters=False,\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Training arguments configured\")\n",
        "print(f\"  Effective batch size: {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}\")\n",
        "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
        "print(f\"  Epochs: {NUM_EPOCHS}\")\n",
        "print(f\"  Total steps: ~{len(dataset) * NUM_EPOCHS // (BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============== INITIALIZE WANDB (Optional) ==============\n",
        "\n",
        "if USE_WANDB:\n",
        "    import wandb\n",
        "    wandb.init(\n",
        "        project=WANDB_PROJECT,\n",
        "        name=f\"deepseek-ocr-invoice-{NUM_EPOCHS}ep\",\n",
        "        config={\n",
        "            \"model\": MODEL_NAME,\n",
        "            \"lora_r\": LORA_R,\n",
        "            \"lora_alpha\": LORA_ALPHA,\n",
        "            \"learning_rate\": LEARNING_RATE,\n",
        "            \"epochs\": NUM_EPOCHS,\n",
        "            \"batch_size\": BATCH_SIZE,\n",
        "            \"gradient_accumulation\": GRADIENT_ACCUMULATION_STEPS,\n",
        "            \"quantization\": \"4bit\" if USE_4BIT_QUANTIZATION else \"bf16\",\n",
        "        }\n",
        "    )\n",
        "    print(\"‚úÖ Weights & Biases initialized\")\n",
        "else:\n",
        "    print(\"‚ÑπÔ∏è Weights & Biases disabled. Set USE_WANDB=True to enable.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============== CREATE TRAINER AND START TRAINING ==============\n",
        "\n",
        "trainer = DeepSeekOCRTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset,\n",
        "    data_collator=collator,\n",
        ")\n",
        "\n",
        "print(\"üöÄ Starting training...\")\n",
        "print(f\"  Dataset size: {len(dataset)} samples\")\n",
        "print(f\"  Using custom DeepSeekOCRTrainer for proper image handling\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Start training\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============== SAVE MODEL ==============\n",
        "\n",
        "print(\"üíæ Saving model...\")\n",
        "\n",
        "# Save the LoRA adapter\n",
        "trainer.save_model(OUTPUT_DIR)\n",
        "tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "\n",
        "print(f\"‚úÖ Model saved to {OUTPUT_DIR}\")\n",
        "print(\"\\nTo load the model later:\")\n",
        "print(f\"\"\"\n",
        "from peft import PeftModel\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"{MODEL_NAME}\",\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "model = PeftModel.from_pretrained(base_model, \"{OUTPUT_DIR}\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"{OUTPUT_DIR}\")\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inference Test\n",
        "\n",
        "Test the fine-tuned model on a sample invoice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============== INFERENCE TEST ==============\n",
        "\n",
        "# Set model to eval mode\n",
        "model.eval()\n",
        "\n",
        "# Test with first sample from dataset\n",
        "test_sample = dataset[0]\n",
        "test_images = [load_image(img_path if os.path.exists(img_path) else os.path.join(IMAGES_DIR, os.path.basename(img_path))) \n",
        "               for img_path in test_sample['images']]\n",
        "\n",
        "print(f\"Testing with invoice: {test_sample['images'][0]}\")\n",
        "print(f\"Number of pages: {len(test_images)}\")\n",
        "\n",
        "# Note: Full inference requires proper handling of images through the model's processor\n",
        "# This is a simplified test - for production use, use the model's built-in chat interface\n",
        "\n",
        "print(\"\\nüìÑ Expected output (first 500 chars):\")\n",
        "print(test_sample['response'][:500] + \"...\")\n",
        "\n",
        "if USE_WANDB:\n",
        "    wandb.finish()\n",
        "    \n",
        "print(\"\\nüéâ Training complete!\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
