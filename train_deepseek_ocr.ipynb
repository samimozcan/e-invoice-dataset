{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b94fc1c",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Environment Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26379f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Python version and GPU\n",
    "import sys\n",
    "print(f\"Python version: {sys.version}\")\n",
    "\n",
    "# Check CUDA availability\n",
    "import subprocess\n",
    "result = subprocess.run(['nvidia-smi', '--query-gpu=name,memory.total,driver_version', '--format=csv'], \n",
    "                       capture_output=True, text=True)\n",
    "print(f\"\\nGPU Info:\\n{result.stdout}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66857661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install all dependencies\n",
    "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "!pip install -q transformers>=4.40.0 accelerate>=0.27.0 datasets>=2.18.0\n",
    "!pip install -q peft>=0.10.0 bitsandbytes>=0.43.0\n",
    "!pip install -q Pillow opencv-python tqdm numpy scipy scikit-learn\n",
    "!pip install -q wandb tensorboard\n",
    "\n",
    "# Install Flash Attention 2 (optional but recommended for speed)\n",
    "!pip install -q flash-attn --no-build-isolation\n",
    "\n",
    "print(\"\\n‚úÖ All dependencies installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d682557",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd62e1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import torch\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Dict, List, Any, Optional\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# Transformers\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoProcessor,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "from transformers.trainer_callback import TrainerCallback\n",
    "\n",
    "# PEFT for LoRA\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n",
    "\n",
    "# Dataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06c4222",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7be5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    \"\"\"Training configuration for DeepSeek OCR fine-tuning\"\"\"\n",
    "    \n",
    "    # Model\n",
    "    model_name: str = \"deepseek-ai/DeepSeek-OCR\"\n",
    "    \n",
    "    # Paths\n",
    "    dataset_path: str = \"dataset_multi_image.jsonl\"\n",
    "    images_dir: str = \"images\"\n",
    "    output_dir: str = \"./deepseek_ocr_finetuned\"\n",
    "    \n",
    "    # Training hyperparameters (optimized for RTX 4090)\n",
    "    num_epochs: int = 10\n",
    "    per_device_train_batch_size: int = 1\n",
    "    per_device_eval_batch_size: int = 1\n",
    "    gradient_accumulation_steps: int = 8  # Effective batch size = 8\n",
    "    learning_rate: float = 2e-4\n",
    "    warmup_ratio: float = 0.1\n",
    "    weight_decay: float = 0.01\n",
    "    max_grad_norm: float = 1.0\n",
    "    \n",
    "    # LoRA config\n",
    "    lora_r: int = 16\n",
    "    lora_alpha: int = 32\n",
    "    lora_dropout: float = 0.05\n",
    "    \n",
    "    # Quantization (4-bit for RTX 4090)\n",
    "    use_4bit: bool = True\n",
    "    bnb_4bit_compute_dtype: str = \"bfloat16\"\n",
    "    bnb_4bit_quant_type: str = \"nf4\"\n",
    "    use_nested_quant: bool = False\n",
    "    \n",
    "    # Other settings\n",
    "    use_flash_attention: bool = True\n",
    "    gradient_checkpointing: bool = True\n",
    "    fp16: bool = False\n",
    "    bf16: bool = True  # Use bf16 for RTX 4090\n",
    "    max_seq_length: int = 4096\n",
    "    seed: int = 42\n",
    "    \n",
    "    # Logging\n",
    "    logging_steps: int = 1\n",
    "    save_steps: int = 50\n",
    "    eval_steps: int = 50\n",
    "    use_wandb: bool = False  # Set to True if you want to use Weights & Biases\n",
    "    wandb_project: str = \"deepseek-ocr-invoice\"\n",
    "\n",
    "config = TrainingConfig()\n",
    "print(\"‚úÖ Configuration loaded:\")\n",
    "print(f\"   Model: {config.model_name}\")\n",
    "print(f\"   Dataset: {config.dataset_path}\")\n",
    "print(f\"   Output: {config.output_dir}\")\n",
    "print(f\"   Epochs: {config.num_epochs}\")\n",
    "print(f\"   Effective batch size: {config.per_device_train_batch_size * config.gradient_accumulation_steps}\")\n",
    "print(f\"   LoRA rank: {config.lora_r}, alpha: {config.lora_alpha}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83703aa9",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Load and Validate Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330f5e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_validate_dataset(dataset_path: str, images_dir: str) -> List[Dict]:\n",
    "    \"\"\"Load JSONL dataset and validate all images exist\"\"\"\n",
    "    \n",
    "    print(f\"üìÇ Loading dataset from: {dataset_path}\")\n",
    "    \n",
    "    samples = []\n",
    "    missing_images = []\n",
    "    \n",
    "    with open(dataset_path, 'r', encoding='utf-8') as f:\n",
    "        for line_num, line in enumerate(f, 1):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                sample = json.loads(line)\n",
    "                \n",
    "                # Validate required fields\n",
    "                if 'images' not in sample or 'prompt' not in sample or 'response' not in sample:\n",
    "                    print(f\"‚ö†Ô∏è Line {line_num}: Missing required fields\")\n",
    "                    continue\n",
    "                \n",
    "                # Check all images exist\n",
    "                valid_images = []\n",
    "                for img_path in sample['images']:\n",
    "                    full_path = Path(img_path)\n",
    "                    if full_path.exists():\n",
    "                        valid_images.append(str(full_path))\n",
    "                    else:\n",
    "                        missing_images.append(img_path)\n",
    "                \n",
    "                if len(valid_images) == len(sample['images']):\n",
    "                    sample['images'] = valid_images\n",
    "                    samples.append(sample)\n",
    "                else:\n",
    "                    print(f\"‚ö†Ô∏è Line {line_num}: Some images missing\")\n",
    "                    \n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"‚ö†Ô∏è Line {line_num}: JSON parse error - {e}\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Dataset loaded successfully!\")\n",
    "    print(f\"   Total samples: {len(samples)}\")\n",
    "    \n",
    "    # Statistics\n",
    "    total_images = sum(len(s['images']) for s in samples)\n",
    "    total_invoices = 0\n",
    "    for s in samples:\n",
    "        try:\n",
    "            resp = json.loads(s['response'])\n",
    "            total_invoices += len(resp.get('data', []))\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    print(f\"   Total images: {total_images}\")\n",
    "    print(f\"   Total invoices: {total_invoices}\")\n",
    "    \n",
    "    if missing_images:\n",
    "        print(f\"\\n‚ö†Ô∏è Missing images ({len(missing_images)}):\")\n",
    "        for img in missing_images[:5]:\n",
    "            print(f\"   - {img}\")\n",
    "        if len(missing_images) > 5:\n",
    "            print(f\"   ... and {len(missing_images) - 5} more\")\n",
    "    \n",
    "    return samples\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_and_validate_dataset(config.dataset_path, config.images_dir)\n",
    "\n",
    "# Show sample structure\n",
    "print(\"\\nüìã Sample entry structure:\")\n",
    "sample = dataset[0]\n",
    "print(f\"   Images: {len(sample['images'])} files\")\n",
    "print(f\"   Prompt length: {len(sample['prompt'])} chars\")\n",
    "print(f\"   Response length: {len(sample['response'])} chars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589cba2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display dataset summary\n",
    "print(\"üìä Dataset Summary:\")\n",
    "print(\"=\" * 80)\n",
    "for i, sample in enumerate(dataset, 1):\n",
    "    num_images = len(sample['images'])\n",
    "    try:\n",
    "        resp = json.loads(sample['response'])\n",
    "        num_invoices = len(resp.get('data', []))\n",
    "    except:\n",
    "        num_invoices = \"?\"\n",
    "    \n",
    "    # Get PDF name from first image\n",
    "    pdf_name = Path(sample['images'][0]).stem.replace('_page_1', '')[:45]\n",
    "    print(f\"{i:2}. {pdf_name:45} | {num_images:2} pages | {num_invoices:2} invoices\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"Total: {len(dataset)} samples ready for training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66009ff",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Create PyTorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfbe2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvoiceOCRDataset(Dataset):\n",
    "    \"\"\"Dataset for DeepSeek OCR fine-tuning on invoices\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        samples: List[Dict], \n",
    "        processor: Any,\n",
    "        tokenizer: Any,\n",
    "        max_length: int = 4096\n",
    "    ):\n",
    "        self.samples = samples\n",
    "        self.processor = processor\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
    "        sample = self.samples[idx]\n",
    "        \n",
    "        # Load images\n",
    "        images = []\n",
    "        for img_path in sample['images']:\n",
    "            try:\n",
    "                img = Image.open(img_path).convert('RGB')\n",
    "                images.append(img)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading image {img_path}: {e}\")\n",
    "                # Create a blank image as fallback\n",
    "                images.append(Image.new('RGB', (224, 224), color='white'))\n",
    "        \n",
    "        # Prepare prompt and response\n",
    "        prompt = sample['prompt']\n",
    "        response = sample['response']\n",
    "        \n",
    "        # Create full conversation\n",
    "        full_text = f\"{prompt}\\n\\n{response}\"\n",
    "        \n",
    "        # Process with processor\n",
    "        try:\n",
    "            inputs = self.processor(\n",
    "                images=images,\n",
    "                text=full_text,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                max_length=self.max_length\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing sample {idx}: {e}\")\n",
    "            # Return a minimal valid tensor\n",
    "            inputs = self.tokenizer(\n",
    "                full_text,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                max_length=self.max_length\n",
    "            )\n",
    "        \n",
    "        # Squeeze batch dimension and prepare labels\n",
    "        result = {}\n",
    "        for key, value in inputs.items():\n",
    "            if isinstance(value, torch.Tensor):\n",
    "                result[key] = value.squeeze(0)\n",
    "            else:\n",
    "                result[key] = value\n",
    "        \n",
    "        # Set labels (same as input_ids for causal LM)\n",
    "        if 'input_ids' in result:\n",
    "            result['labels'] = result['input_ids'].clone()\n",
    "            # Mask prompt tokens (only train on response)\n",
    "            prompt_tokens = self.tokenizer.encode(prompt, add_special_tokens=False)\n",
    "            prompt_len = len(prompt_tokens)\n",
    "            result['labels'][:prompt_len] = -100  # Ignore prompt in loss\n",
    "        \n",
    "        return result\n",
    "\n",
    "print(\"‚úÖ Dataset class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5762ae4",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Load Model with QLoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f4c910",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_processor(config: TrainingConfig):\n",
    "    \"\"\"Load DeepSeek OCR model with 4-bit quantization\"\"\"\n",
    "    \n",
    "    print(f\"üì• Loading model: {config.model_name}\")\n",
    "    print(\"   This may take a few minutes...\")\n",
    "    \n",
    "    # BitsAndBytes config for 4-bit quantization\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=config.use_4bit,\n",
    "        bnb_4bit_quant_type=config.bnb_4bit_quant_type,\n",
    "        bnb_4bit_compute_dtype=getattr(torch, config.bnb_4bit_compute_dtype),\n",
    "        bnb_4bit_use_double_quant=config.use_nested_quant,\n",
    "    )\n",
    "    \n",
    "    # Load processor\n",
    "    print(\"   Loading processor...\")\n",
    "    processor = AutoProcessor.from_pretrained(\n",
    "        config.model_name,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    # Load tokenizer\n",
    "    print(\"   Loading tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        config.model_name,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    # Ensure special tokens are set\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    if tokenizer.bos_token is None:\n",
    "        tokenizer.bos_token = tokenizer.eos_token\n",
    "    \n",
    "    # Load model with quantization\n",
    "    print(\"   Loading model with 4-bit quantization...\")\n",
    "    model_kwargs = {\n",
    "        \"quantization_config\": bnb_config,\n",
    "        \"device_map\": \"auto\",\n",
    "        \"trust_remote_code\": True,\n",
    "        \"torch_dtype\": torch.bfloat16,\n",
    "    }\n",
    "    \n",
    "    # Add flash attention if available\n",
    "    if config.use_flash_attention:\n",
    "        try:\n",
    "            model_kwargs[\"attn_implementation\"] = \"flash_attention_2\"\n",
    "            print(\"   Using Flash Attention 2\")\n",
    "        except:\n",
    "            print(\"   Flash Attention not available, using default\")\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        config.model_name,\n",
    "        **model_kwargs\n",
    "    )\n",
    "    \n",
    "    # Prepare for k-bit training\n",
    "    model = prepare_model_for_kbit_training(\n",
    "        model,\n",
    "        use_gradient_checkpointing=config.gradient_checkpointing\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n‚úÖ Model loaded successfully!\")\n",
    "    print(f\"   Model dtype: {model.dtype}\")\n",
    "    print(f\"   Model device: {model.device}\")\n",
    "    \n",
    "    # Memory usage\n",
    "    if torch.cuda.is_available():\n",
    "        memory_used = torch.cuda.memory_allocated() / 1e9\n",
    "        memory_total = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "        print(f\"   GPU memory: {memory_used:.1f}GB / {memory_total:.1f}GB\")\n",
    "    \n",
    "    return model, processor, tokenizer\n",
    "\n",
    "# Load model\n",
    "model, processor, tokenizer = load_model_and_processor(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66ab321",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Apply LoRA Adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16560af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_lora(model, config: TrainingConfig):\n",
    "    \"\"\"Apply LoRA adapters to the model\"\"\"\n",
    "    \n",
    "    print(\"üîß Applying LoRA adapters...\")\n",
    "    \n",
    "    # Find all linear layer names (excluding lm_head)\n",
    "    target_modules = []\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            if 'lm_head' not in name:\n",
    "                # Get just the last part of the name\n",
    "                layer_name = name.split('.')[-1]\n",
    "                if layer_name not in target_modules:\n",
    "                    target_modules.append(layer_name)\n",
    "    \n",
    "    # Common target modules for transformer models\n",
    "    common_targets = ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']\n",
    "    target_modules = [m for m in common_targets if any(m in name for name, _ in model.named_modules())]\n",
    "    \n",
    "    if not target_modules:\n",
    "        target_modules = ['q_proj', 'v_proj']  # Fallback\n",
    "    \n",
    "    print(f\"   Target modules: {target_modules}\")\n",
    "    \n",
    "    # LoRA configuration\n",
    "    lora_config = LoraConfig(\n",
    "        r=config.lora_r,\n",
    "        lora_alpha=config.lora_alpha,\n",
    "        lora_dropout=config.lora_dropout,\n",
    "        bias=\"none\",\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        target_modules=target_modules,\n",
    "    )\n",
    "    \n",
    "    # Apply LoRA\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    \n",
    "    # Print trainable parameters\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_percent = 100 * trainable_params / total_params\n",
    "    \n",
    "    print(f\"\\n‚úÖ LoRA applied successfully!\")\n",
    "    print(f\"   Trainable parameters: {trainable_params:,} ({trainable_percent:.2f}%)\")\n",
    "    print(f\"   Total parameters: {total_params:,}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Apply LoRA\n",
    "model = apply_lora(model, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d8a0b6",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Create Data Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdd87dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataCollatorForOCR:\n",
    "    \"\"\"Custom data collator for OCR training with multi-image support\"\"\"\n",
    "    \n",
    "    def __init__(self, tokenizer, pad_token_id: int = None):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.pad_token_id = pad_token_id or tokenizer.pad_token_id\n",
    "        \n",
    "    def __call__(self, features: List[Dict]) -> Dict[str, torch.Tensor]:\n",
    "        # Find all keys\n",
    "        all_keys = set()\n",
    "        for f in features:\n",
    "            all_keys.update(f.keys())\n",
    "        \n",
    "        batch = {}\n",
    "        \n",
    "        for key in all_keys:\n",
    "            values = [f.get(key) for f in features if key in f]\n",
    "            \n",
    "            if not values:\n",
    "                continue\n",
    "                \n",
    "            if isinstance(values[0], torch.Tensor):\n",
    "                # Stack tensors\n",
    "                if all(v.shape == values[0].shape for v in values):\n",
    "                    batch[key] = torch.stack(values)\n",
    "                else:\n",
    "                    # Pad tensors to same length\n",
    "                    max_len = max(v.shape[0] for v in values)\n",
    "                    padded = []\n",
    "                    for v in values:\n",
    "                        if v.shape[0] < max_len:\n",
    "                            pad_size = max_len - v.shape[0]\n",
    "                            if key == 'labels':\n",
    "                                pad_value = -100\n",
    "                            elif key == 'attention_mask':\n",
    "                                pad_value = 0\n",
    "                            else:\n",
    "                                pad_value = self.pad_token_id\n",
    "                            v = torch.cat([v, torch.full((pad_size,) + v.shape[1:], pad_value, dtype=v.dtype)])\n",
    "                        padded.append(v)\n",
    "                    batch[key] = torch.stack(padded)\n",
    "            elif isinstance(values[0], list):\n",
    "                batch[key] = values\n",
    "            else:\n",
    "                batch[key] = values\n",
    "        \n",
    "        return batch\n",
    "\n",
    "# Create collator\n",
    "data_collator = DataCollatorForOCR(tokenizer)\n",
    "print(\"‚úÖ Data collator created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70fb6890",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d805aead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into train/val\n",
    "train_samples, val_samples = train_test_split(\n",
    "    dataset, \n",
    "    test_size=0.1,  # 10% for validation\n",
    "    random_state=config.seed\n",
    ")\n",
    "\n",
    "print(f\"üìä Dataset split:\")\n",
    "print(f\"   Training samples: {len(train_samples)}\")\n",
    "print(f\"   Validation samples: {len(val_samples)}\")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = InvoiceOCRDataset(\n",
    "    samples=train_samples,\n",
    "    processor=processor,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=config.max_seq_length\n",
    ")\n",
    "\n",
    "val_dataset = InvoiceOCRDataset(\n",
    "    samples=val_samples,\n",
    "    processor=processor,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=config.max_seq_length\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Datasets created:\")\n",
    "print(f\"   Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"   Val dataset size: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a810e130",
   "metadata": {},
   "source": [
    "## üîü Setup Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32d0aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=config.output_dir,\n",
    "    num_train_epochs=config.num_epochs,\n",
    "    per_device_train_batch_size=config.per_device_train_batch_size,\n",
    "    per_device_eval_batch_size=config.per_device_eval_batch_size,\n",
    "    gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "    learning_rate=config.learning_rate,\n",
    "    warmup_ratio=config.warmup_ratio,\n",
    "    weight_decay=config.weight_decay,\n",
    "    max_grad_norm=config.max_grad_norm,\n",
    "    \n",
    "    # Precision\n",
    "    fp16=config.fp16,\n",
    "    bf16=config.bf16,\n",
    "    \n",
    "    # Logging\n",
    "    logging_dir=f\"{config.output_dir}/logs\",\n",
    "    logging_steps=config.logging_steps,\n",
    "    logging_first_step=True,\n",
    "    \n",
    "    # Evaluation & Saving\n",
    "    evaluation_strategy=\"steps\" if len(val_samples) > 0 else \"no\",\n",
    "    eval_steps=config.eval_steps,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=config.save_steps,\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True if len(val_samples) > 0 else False,\n",
    "    \n",
    "    # Optimization\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    \n",
    "    # Other\n",
    "    gradient_checkpointing=config.gradient_checkpointing,\n",
    "    dataloader_pin_memory=True,\n",
    "    remove_unused_columns=False,\n",
    "    report_to=\"wandb\" if config.use_wandb else \"tensorboard\",\n",
    "    run_name=f\"deepseek-ocr-invoice-{config.seed}\",\n",
    "    seed=config.seed,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Training arguments configured\")\n",
    "print(f\"   Output directory: {training_args.output_dir}\")\n",
    "print(f\"   Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"   Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"   Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"   Gradient accumulation: {training_args.gradient_accumulation_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499a9427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom trainer for multi-image OCR\n",
    "class DeepSeekOCRTrainer(Trainer):\n",
    "    \"\"\"Custom trainer that properly handles multi-image inputs\"\"\"\n",
    "    \n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        \"\"\"\n",
    "        Override compute_loss to handle the model's output format.\n",
    "        \"\"\"\n",
    "        labels = inputs.pop(\"labels\", None)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "        if labels is not None:\n",
    "            # Get logits\n",
    "            logits = outputs.logits if hasattr(outputs, 'logits') else outputs[0]\n",
    "            \n",
    "            # Shift for causal LM\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            \n",
    "            # Compute loss\n",
    "            loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
    "            loss = loss_fct(\n",
    "                shift_logits.view(-1, shift_logits.size(-1)),\n",
    "                shift_labels.view(-1)\n",
    "            )\n",
    "        else:\n",
    "            loss = outputs.loss if hasattr(outputs, 'loss') else outputs[0]\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# Create trainer\n",
    "trainer = DeepSeekOCRTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset if len(val_samples) > 0 else None,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Trainer created and ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea0a9dc",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£1Ô∏è‚É£ Train the Model! üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe12c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize wandb if enabled\n",
    "if config.use_wandb:\n",
    "    import wandb\n",
    "    wandb.init(\n",
    "        project=config.wandb_project,\n",
    "        name=f\"deepseek-ocr-{config.seed}\",\n",
    "        config=vars(config)\n",
    "    )\n",
    "\n",
    "print(\"üöÄ Starting training...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Clear CUDA cache before training\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"GPU memory before training: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "\n",
    "# Train!\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"‚úÖ Training completed!\")\n",
    "print(f\"   Total steps: {train_result.global_step}\")\n",
    "print(f\"   Training loss: {train_result.training_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2031cda",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£2Ô∏è‚É£ Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63bc002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final model\n",
    "print(\"üíæ Saving model...\")\n",
    "\n",
    "# Save LoRA adapters\n",
    "lora_output_dir = f\"{config.output_dir}/lora_adapters\"\n",
    "trainer.model.save_pretrained(lora_output_dir)\n",
    "tokenizer.save_pretrained(lora_output_dir)\n",
    "\n",
    "print(f\"\\n‚úÖ Model saved to: {lora_output_dir}\")\n",
    "print(\"\\nüìÅ Saved files:\")\n",
    "for f in Path(lora_output_dir).iterdir():\n",
    "    print(f\"   - {f.name}\")\n",
    "\n",
    "# Save training metrics\n",
    "metrics_file = f\"{config.output_dir}/training_metrics.json\"\n",
    "with open(metrics_file, 'w') as f:\n",
    "    json.dump({\n",
    "        \"training_loss\": train_result.training_loss,\n",
    "        \"global_step\": train_result.global_step,\n",
    "        \"epochs\": config.num_epochs,\n",
    "        \"samples\": len(train_samples),\n",
    "    }, f, indent=2)\n",
    "print(f\"   - training_metrics.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f27f72",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£3Ô∏è‚É£ Test Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031debfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test inference on a sample\n",
    "print(\"üß™ Testing inference on a sample...\")\n",
    "\n",
    "# Get a test sample\n",
    "test_sample = dataset[0]\n",
    "\n",
    "# Load images\n",
    "test_images = [Image.open(img_path).convert('RGB') for img_path in test_sample['images'][:3]]  # Limit to 3 for testing\n",
    "\n",
    "# Extract just the prompt (without schema for cleaner input)\n",
    "prompt = test_sample['prompt']\n",
    "\n",
    "print(f\"\\nTest sample info:\")\n",
    "print(f\"   Number of images: {len(test_images)}\")\n",
    "print(f\"   Prompt length: {len(prompt)} chars\")\n",
    "\n",
    "# Process inputs\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    inputs = processor(\n",
    "        images=test_images,\n",
    "        text=prompt,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "    \n",
    "    # Generate\n",
    "    print(\"\\n‚è≥ Generating output...\")\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=2048,\n",
    "        do_sample=False,\n",
    "        num_beams=1,\n",
    "    )\n",
    "    \n",
    "    # Decode\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract just the response part\n",
    "    if prompt in generated_text:\n",
    "        response = generated_text.split(prompt)[-1].strip()\n",
    "    else:\n",
    "        response = generated_text\n",
    "\n",
    "print(\"\\nüìÑ Generated output (first 1000 chars):\")\n",
    "print(\"-\" * 50)\n",
    "print(response[:1000])\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Try to parse as JSON\n",
    "try:\n",
    "    parsed = json.loads(response)\n",
    "    print(\"\\n‚úÖ Output is valid JSON!\")\n",
    "except json.JSONDecodeError:\n",
    "    print(\"\\n‚ö†Ô∏è Output is not valid JSON (may need more training)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3553bd8",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£4Ô∏è‚É£ Cleanup and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f2caee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üéâ TRAINING COMPLETE!\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\"\"\n",
    "üìä Training Summary:\n",
    "   - Model: {config.model_name}\n",
    "   - Training samples: {len(train_samples)}\n",
    "   - Validation samples: {len(val_samples)}\n",
    "   - Epochs: {config.num_epochs}\n",
    "   - Final loss: {train_result.training_loss:.4f}\n",
    "   - Total steps: {train_result.global_step}\n",
    "\n",
    "üìÅ Output files:\n",
    "   - LoRA adapters: {lora_output_dir}\n",
    "   - Checkpoints: {config.output_dir}\n",
    "   - Logs: {config.output_dir}/logs\n",
    "\n",
    "üöÄ To load the fine-tuned model:\n",
    "   from peft import PeftModel\n",
    "   base_model = AutoModelForCausalLM.from_pretrained(\"{config.model_name}\")\n",
    "   model = PeftModel.from_pretrained(base_model, \"{lora_output_dir}\")\n",
    "\"\"\")\n",
    "\n",
    "# Clean up\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"\\nüíæ Final GPU memory: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "\n",
    "# Close wandb if used\n",
    "if config.use_wandb:\n",
    "    wandb.finish()\n",
    "    print(\"üìä WandB run finished\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
