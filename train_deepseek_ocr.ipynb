{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DeepSeek OCR Training Notebook\n",
        "\n",
        "This notebook demonstrates how to fine-tune the DeepSeek OCR model using the dataset created in the previous steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install requirements\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import Trainer, TrainingArguments, AutoTokenizer, AutoModelForCausalLM, AutoConfig, AutoModel\n",
        "from PIL import Image, ImageOps\n",
        "from torchvision import transforms\n",
        "from typing import List, Dict, Optional, Tuple, Union\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Helper Functions\n",
        "\n",
        "These functions are extracted from the DeepSeek OCR model code to handle image preprocessing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_image(image_path):\n",
        "    try:\n",
        "        image = Image.open(image_path)\n",
        "        corrected_image = ImageOps.exif_transpose(image)\n",
        "        return corrected_image\n",
        "    except Exception as e:\n",
        "        print(f\"error loading image {image_path}: {e}\")\n",
        "        try:\n",
        "            return Image.open(image_path)\n",
        "        except:\n",
        "            return None\n",
        "\n",
        "def find_closest_aspect_ratio(aspect_ratio, target_ratios, width, height, image_size):\n",
        "    best_ratio_diff = float('inf')\n",
        "    best_ratio = (1, 1)\n",
        "    area = width * height\n",
        "    for ratio in target_ratios:\n",
        "        target_aspect_ratio = ratio[0] / ratio[1]\n",
        "        ratio_diff = abs(aspect_ratio - target_aspect_ratio)\n",
        "        if ratio_diff < best_ratio_diff:\n",
        "            best_ratio_diff = ratio_diff\n",
        "            best_ratio = ratio\n",
        "        elif ratio_diff == best_ratio_diff:\n",
        "            if area > 0.5 * image_size * image_size * ratio[0] * ratio[1]:\n",
        "                best_ratio = ratio\n",
        "    return best_ratio\n",
        "\n",
        "def dynamic_preprocess(image, min_num=1, max_num=9, image_size=640, use_thumbnail=False):\n",
        "    orig_width, orig_height = image.size\n",
        "    aspect_ratio = orig_width / orig_height\n",
        "\n",
        "    target_ratios = set(\n",
        "        (i, j) for n in range(min_num, max_num + 1) for i in range(1, n + 1) for j in range(1, n + 1) if\n",
        "        i * j <= max_num and i * j >= min_num)\n",
        "    target_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])\n",
        "\n",
        "    target_aspect_ratio = find_closest_aspect_ratio(\n",
        "        aspect_ratio, target_ratios, orig_width, orig_height, image_size)\n",
        "\n",
        "    target_width = image_size * target_aspect_ratio[0]\n",
        "    target_height = image_size * target_aspect_ratio[1]\n",
        "    blocks = target_aspect_ratio[0] * target_aspect_ratio[1]\n",
        "\n",
        "    resized_img = image.resize((target_width, target_height))\n",
        "    processed_images = []\n",
        "    for i in range(blocks):\n",
        "        box = (\n",
        "            (i % (target_width // image_size)) * image_size,\n",
        "            (i // (target_width // image_size)) * image_size,\n",
        "            ((i % (target_width // image_size)) + 1) * image_size,\n",
        "            ((i // (target_width // image_size)) + 1) * image_size\n",
        "        )\n",
        "        split_img = resized_img.crop(box)\n",
        "        processed_images.append(split_img)\n",
        "    assert len(processed_images) == blocks\n",
        "    if use_thumbnail and len(processed_images) != 1:\n",
        "        thumbnail_img = image.resize((image_size, image_size))\n",
        "        processed_images.append(thumbnail_img)\n",
        "    return processed_images, target_aspect_ratio\n",
        "\n",
        "def normalize_transform(mean, std):\n",
        "    if mean is None and std is None:\n",
        "        transform = None\n",
        "    elif mean is None and std is not None:\n",
        "        mean = [0.] * len(std)\n",
        "        transform = transforms.Normalize(mean=mean, std=std)\n",
        "    elif mean is not None and std is None:\n",
        "        std = [1.] * len(mean)\n",
        "        transform = transforms.Normalize(mean=mean, std=std)\n",
        "    else:\n",
        "        transform = transforms.Normalize(mean=mean, std=std)\n",
        "    return transform\n",
        "\n",
        "class BasicImageTransform:\n",
        "    def __init__(\n",
        "        self, \n",
        "        mean: Optional[Tuple[float, float, float]] = (0.5, 0.5, 0.5),\n",
        "        std: Optional[Tuple[float, float, float]] = (0.5, 0.5, 0.5),\n",
        "        normalize: bool = True\n",
        "    ):\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "        transform_pipelines = [transforms.ToTensor()]\n",
        "        normalize_t = normalize_transform(mean, std) if normalize else nn.Identity()\n",
        "        if normalize_t is not None:\n",
        "            transform_pipelines.append(normalize_t)\n",
        "        self.transform = transforms.Compose(transform_pipelines)\n",
        "    \n",
        "    def __call__(self, x):\n",
        "        return self.transform(x)\n",
        "\n",
        "def text_encode(tokenizer, text: str, bos: bool = True, eos: bool = False):\n",
        "    t = tokenizer.encode(text, add_special_tokens=False)\n",
        "    bos_id = tokenizer.bos_token_id if tokenizer.bos_token_id is not None else 1\n",
        "    eos_id = tokenizer.eos_token_id if tokenizer.eos_token_id is not None else 2\n",
        "    \n",
        "    if bos:\n",
        "        t = [bos_id] + t\n",
        "    if eos:\n",
        "        t = t + [eos_id]\n",
        "    return t"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset and Collator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DeepSeekOCRDataset(Dataset):\n",
        "    def __init__(self, data_path):\n",
        "        self.data = []\n",
        "        with open(data_path, 'r') as f:\n",
        "            for line in f:\n",
        "                self.data.append(json.loads(line))\n",
        "                \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "class DeepSeekOCRDataCollator:\n",
        "    def __init__(self, tokenizer, image_size=640, base_size=1024, patch_size=16, downsample_ratio=4):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.image_size = image_size\n",
        "        self.base_size = base_size\n",
        "        self.patch_size = patch_size\n",
        "        self.downsample_ratio = downsample_ratio\n",
        "        self.image_transform = BasicImageTransform(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5), normalize=True)\n",
        "        self.image_token = '<image>'\n",
        "        self.image_token_id = tokenizer.convert_tokens_to_ids(self.image_token)\n",
        "        if self.image_token_id is None:\n",
        "             self.image_token_id = 128815\n",
        "             \n",
        "    def __call__(self, batch):\n",
        "        input_ids_batch = []\n",
        "        labels_batch = []\n",
        "        images_batch = []\n",
        "        images_seq_mask_batch = []\n",
        "        images_spatial_crop_batch = []\n",
        "        \n",
        "        for item in batch:\n",
        "            prompt = item['prompt']\n",
        "            response = item['response']\n",
        "            image_paths = item['images']\n",
        "            \n",
        "            images = []\n",
        "            for img_path in image_paths:\n",
        "                img = load_image(img_path)\n",
        "                if img:\n",
        "                    images.append(img.convert(\"RGB\"))\n",
        "            \n",
        "            text_splits = prompt.split(self.image_token)\n",
        "            \n",
        "            tokenized_str = []\n",
        "            images_seq_mask = []\n",
        "            images_list = []\n",
        "            images_crop_list = []\n",
        "            \n",
        "            current_images_spatial_crop = []\n",
        "            \n",
        "            for i, text_sep in enumerate(text_splits):\n",
        "                tokenized_sep = text_encode(self.tokenizer, text_sep, bos=False, eos=False)\n",
        "                tokenized_str += tokenized_sep\n",
        "                images_seq_mask += [False] * len(tokenized_sep)\n",
        "                \n",
        "                if i < len(images):\n",
        "                    image = images[i]\n",
        "                    \n",
        "                    images_crop_raw, crop_ratio = dynamic_preprocess(image, image_size=self.image_size)\n",
        "                    \n",
        "                    global_view = ImageOps.pad(image, (self.base_size, self.base_size),\n",
        "                                            color=tuple(int(x * 255) for x in self.image_transform.mean))\n",
        "                    images_list.append(self.image_transform(global_view).to(torch.bfloat16))\n",
        "                    \n",
        "                    width_crop_num, height_crop_num = crop_ratio\n",
        "                    current_images_spatial_crop.append([width_crop_num, height_crop_num])\n",
        "                    \n",
        "                    if width_crop_num > 1 or height_crop_num > 1:\n",
        "                        for crop_img in images_crop_raw:\n",
        "                            images_crop_list.append(self.image_transform(crop_img).to(torch.bfloat16))\n",
        "                            \n",
        "                    num_queries = math.ceil((self.image_size // self.patch_size) / self.downsample_ratio)\n",
        "                    num_queries_base = math.ceil((self.base_size // self.patch_size) / self.downsample_ratio)\n",
        "                    \n",
        "                    tokenized_image = ([self.image_token_id] * num_queries_base + [self.image_token_id]) * num_queries_base\n",
        "                    tokenized_image += [self.image_token_id]\n",
        "                    \n",
        "                    if width_crop_num > 1 or height_crop_num > 1:\n",
        "                        tokenized_image += ([self.image_token_id] * (num_queries * width_crop_num) + [self.image_token_id]) * (\n",
        "                                    num_queries * height_crop_num)\n",
        "                                    \n",
        "                    tokenized_str += tokenized_image\n",
        "                    images_seq_mask += [True] * len(tokenized_image)\n",
        "            \n",
        "            bos_id = 0\n",
        "            tokenized_str = [bos_id] + tokenized_str\n",
        "            images_seq_mask = [False] + images_seq_mask\n",
        "            \n",
        "            response_tokens = text_encode(self.tokenizer, response, bos=False, eos=True)\n",
        "            \n",
        "            input_ids = tokenized_str + response_tokens\n",
        "            images_seq_mask += [False] * len(response_tokens)\n",
        "            \n",
        "            labels = [-100] * len(tokenized_str) + response_tokens\n",
        "            \n",
        "            input_ids_batch.append(torch.LongTensor(input_ids))\n",
        "            labels_batch.append(torch.LongTensor(labels))\n",
        "            images_seq_mask_batch.append(torch.tensor(images_seq_mask, dtype=torch.bool))\n",
        "            \n",
        "            if len(images_list) > 0:\n",
        "                images_ori = torch.stack(images_list, dim=0)\n",
        "                images_spatial_crop_tensor = torch.tensor(current_images_spatial_crop, dtype=torch.long)\n",
        "                if images_crop_list:\n",
        "                    images_crop = torch.stack(images_crop_list, dim=0)\n",
        "                else:\n",
        "                    images_crop = torch.zeros((1, 3, self.base_size, self.base_size)).to(torch.bfloat16)\n",
        "                \n",
        "                images_batch.append((images_crop, images_ori))\n",
        "                images_spatial_crop_batch.append(images_spatial_crop_tensor)\n",
        "\n",
        "        input_ids_padded = torch.nn.utils.rnn.pad_sequence(input_ids_batch, batch_first=True, padding_value=self.tokenizer.pad_token_id)\n",
        "        labels_padded = torch.nn.utils.rnn.pad_sequence(labels_batch, batch_first=True, padding_value=-100)\n",
        "        images_seq_mask_padded = torch.nn.utils.rnn.pad_sequence(images_seq_mask_batch, batch_first=True, padding_value=False)\n",
        "        \n",
        "        return {\n",
        "            \"input_ids\": input_ids_padded,\n",
        "            \"labels\": labels_padded,\n",
        "            \"images\": images_batch,\n",
        "            \"images_seq_mask\": images_seq_mask_padded,\n",
        "            \"images_spatial_crop\": images_spatial_crop_batch\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_name = \"deepseek-ai/DeepSeek-OCR\"\n",
        "output_dir = \"deepseek_ocr_finetune\"\n",
        "\n",
        "print(f\"Loading model {model_name}...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name, \n",
        "    trust_remote_code=True, \n",
        "    torch_dtype=torch.bfloat16,\n",
        "    _attn_implementation='flash_attention_2'\n",
        ")\n",
        "\n",
        "model.gradient_checkpointing_enable()\n",
        "\n",
        "dataset = DeepSeekOCRDataset(\"dataset_multi_image.jsonl\")\n",
        "collator = DeepSeekOCRDataCollator(tokenizer)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=4,\n",
        "    num_train_epochs=3,\n",
        "    learning_rate=2e-5,\n",
        "    logging_steps=10,\n",
        "    save_steps=100,\n",
        "    bf16=True,\n",
        "    dataloader_pin_memory=False,\n",
        "    remove_unused_columns=False,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset,\n",
        "    data_collator=collator\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Starting training...\")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Saving model...\")\n",
        "trainer.save_model(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
